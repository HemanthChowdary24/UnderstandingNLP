{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# SpaCy Library"
      ],
      "metadata": {
        "id": "nbLcuVCBVmH0"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "spaCy is an open-Source library which is fast, efficient.  \n",
        "it is written in Python and Cython  \n",
        "### Q) How is spaCy different from NLTK?  \n",
        "-spaCy is faster, Scalable and is used for industrial and production purposes whereas NLTK is slower because of it's modularity and is used for academic and research purposes.  \n",
        "-spaCy has built-in functions and libraries which can be used to perform text preprocessing steps like POS tagging, NER.  \n",
        "-tokenization is rule Based in spacy whereas we have methods to perform tokenization in NLTK.  \n",
        "-spaCy supports word vectors wheras we have limited support in NLTK"
      ],
      "metadata": {
        "id": "YcSTNgpPVsxt"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Q) What are the main components of spaCy objects  \n",
        "#### doc\n",
        " - used as a container to hold processed text with tokens and annonations\n",
        "\n",
        "#### Token\n",
        " - represents a single word\n",
        "\n",
        "#### span\n",
        "  - represnts a slice of doc"
      ],
      "metadata": {
        "id": "hHlFRsqmae4V"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "## üîπ Customizing Tokenization Rules in spaCy\n",
        "\n",
        "# You can customize tokenization in spaCy by modifying the Tokenizer settings.\n",
        "\n",
        "### 1Ô∏è‚É£ Customizing Tokenizer Prefixes, Suffixes, and Infixes\n",
        "import spacy\n",
        "from spacy.tokenizer import Tokenizer\n",
        "from spacy.util import compile_infix_regex\n",
        "\n",
        "nlp = spacy.load(\"en_core_web_sm\")\n",
        "infixes = (\"-\", \"\\.\")  # Custom infix rules\n",
        "infix_re = compile_infix_regex(infixes)\n",
        "\n",
        "nlp.tokenizer = Tokenizer(nlp.vocab, infix_finditer=infix_re.finditer)\n",
        "doc = nlp(\"custom-tokenization example\")\n",
        "print([token.text for token in doc])\n",
        "\n",
        "### 2Ô∏è‚É£ Adding Special Cases to the Tokenizer\n",
        "from spacy.symbols import ORTH\n",
        "\n",
        "special_case = [{ORTH: \"spaCy\"}]\n",
        "nlp.tokenizer.add_special_case(\"spaCy\", special_case)\n",
        "doc = nlp(\"I love spaCy!\")\n",
        "print([token.text for token in doc])\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "38ddHImxcnPO",
        "outputId": "71afa4b8-b88d-4554-de9d-fbf772cb0436"
      },
      "execution_count": 54,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['custom', '-', 'tokenization', 'example']\n",
            "['I', 'love', 'spaCy!']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Experiment 1"
      ],
      "metadata": {
        "id": "ORT_xnvhfPCL"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "question: Create a basic NLP program to find words, phrases, names and concepts using \"spacy.blank\" to create the English nlp object. Process the text and instantiate a Doc object in the variable doc. Select the first token of the Doc and print its text.  \n"
      ],
      "metadata": {
        "id": "7lQL9rvAfTKg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#import the spacy library\n",
        "import spacy"
      ],
      "metadata": {
        "id": "9fWeKnBbfQqk"
      },
      "execution_count": 27,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#create a blank NLP object\n",
        "nlp = spacy.load(\"en_core_web_sm\")"
      ],
      "metadata": {
        "id": "mgATU2SXgKgU"
      },
      "execution_count": 35,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "text = \"I love... visiting New-York in the summer. The best time of the year.\""
      ],
      "metadata": {
        "id": "MpN0mgNEgZ1v"
      },
      "execution_count": 36,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# send the text to NLP object and the doc object holds the processed text\n",
        "doc = nlp(text)"
      ],
      "metadata": {
        "id": "rmLEEG91g01_"
      },
      "execution_count": 37,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# to access the first element/token use obj[0] which will return the first token\n",
        "first_token = doc[0]\n",
        "print(\"the first token is -\",first_token)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CMEKxxiyhKD_",
        "outputId": "5c74c3db-1f20-4418-be6e-89055f78e730"
      },
      "execution_count": 38,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "the first token is - I\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#to print all the tokens in the given text\n",
        "print([token.text for token in doc])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "R3ZWos1UhjM5",
        "outputId": "0c0f0937-e01b-42dd-cd51-5398e161db9c"
      },
      "execution_count": 39,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['I', 'love', '...', 'visiting', 'New', '-', 'York', 'in', 'the', 'summer', '.', 'The', 'best', 'time', 'of', 'the', 'year', '.']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#extract words while ignoring punctuations\n",
        "words = [token.text for token in doc if not token.is_punct]\n",
        "print(\"Words:\", words)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XhvHpiYOiz71",
        "outputId": "1c729a22-fd9c-4e41-b6cd-2a731d900537"
      },
      "execution_count": 40,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Words: ['I', 'love', 'visiting', 'New', 'York', 'in', 'the', 'summer', 'The', 'best', 'time', 'of', 'the', 'year']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#extract sentences from the doc object\n",
        "sentences = [sent.text for sent in doc.sents]\n",
        "print(\"Sentences:\", sentences)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UpQisAzkjRcI",
        "outputId": "76d5ea85-ee78-4317-b07d-5acbb77f226b"
      },
      "execution_count": 41,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Sentences: ['I love... visiting New-York in the summer.', 'The best time of the year.']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Extract parts of speech from the text\n",
        "for token in doc:\n",
        "  print(f\"TOKEN:{token},POS:{token.pos_}\",end=\"\\n\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2_jXLVnijsnT",
        "outputId": "58bd1d01-facb-40ce-bb1e-6dfeebd4f471"
      },
      "execution_count": 47,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "TOKEN:I,POS:PRON\n",
            "TOKEN:love,POS:VERB\n",
            "TOKEN:...,POS:PUNCT\n",
            "TOKEN:visiting,POS:VERB\n",
            "TOKEN:New,POS:PROPN\n",
            "TOKEN:-,POS:PUNCT\n",
            "TOKEN:York,POS:PROPN\n",
            "TOKEN:in,POS:ADP\n",
            "TOKEN:the,POS:DET\n",
            "TOKEN:summer,POS:NOUN\n",
            "TOKEN:.,POS:PUNCT\n",
            "TOKEN:The,POS:DET\n",
            "TOKEN:best,POS:ADJ\n",
            "TOKEN:time,POS:NOUN\n",
            "TOKEN:of,POS:ADP\n",
            "TOKEN:the,POS:DET\n",
            "TOKEN:year,POS:NOUN\n",
            "TOKEN:.,POS:PUNCT\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#extract named entities fro the text\n",
        "for ent in doc.ents:\n",
        "    print(f\"Entity: {ent.text}, Label: {ent.label_}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GVzUXgOunzm1",
        "outputId": "787eca1a-d2c8-405c-9357-fc2633064863"
      },
      "execution_count": 51,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Entity: New-York, Label: GPE\n",
            "Entity: the summer, Label: DATE\n",
            "Entity: the year, Label: DATE\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Extract the stop words from the text\n",
        "stop_words = [token.text for token in doc if token.is_stop]\n",
        "print(\"Stop words are: \",stop_words)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ky5SzJLqk8g1",
        "outputId": "269fa294-c0d8-4779-9205-14f211e9b8b6"
      },
      "execution_count": 48,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Stop words are:  ['I', 'in', 'the', 'The', 'of', 'the']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#generate a text without the stop words\n",
        "non_stop_words = [token.text for token in doc if not token.is_stop]\n",
        "filtered=' '.join(non_stop_words)\n",
        "print(f\"original text: {text}\")\n",
        "print(f\"Filtered text: {}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1_uNj6NBmKxt",
        "outputId": "5b8aca62-7206-421e-9da7-5174eec80215"
      },
      "execution_count": 50,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "original text: I love... visiting New-York in the summer. The best time of the year.\n",
            "Filtered text: love ... visiting New - York summer . best time year .\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "BacPsPjgmhsx"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}